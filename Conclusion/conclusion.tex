%*******************************************************************************
%*********************************** First Introduction *****************************
%*******************************************************************************
%\chapter{Introduction}  %Title of the First Introduction
%\chapter*{Introduction}
\chapter{Conclusion}  %Title of the First Introduction
%\addcontentsline{tableofcontents}{chapter}{introduction}

\ifpdf
    \graphicspath{{Conclusion/Figs/Raster/}{Conclusion/Figs/PDF/}{Conclusion/Figs/}}
\else
    \graphicspath{{Conclusion/Figs/Vector/}{Conclusion/Figs/}}
\fi

%********************************** %First Section  **************************************
\section{Conclusion} %Section - 1.1 	
We have explored the possibility of adapting an event-extraction system in relation extraction tasks for the Variome Corpus. 

\section{Contributions}
The contributions of the research in this thesis are as follows:
\begin{itemize}
	\item \emph{The first attempt to use the system outside original system developers.} While this may seem like an easy task, a lot of the time for this project has been devoted to resolving dependencies, and debugging scripts to first have an end-to-end system, even for the original shared task data set.
	\item \emph{The first attempt for literature mining of the Variome Corpus.} To our knowledge, this is the first text mining attempt to the Variome Corpus. 
\end{itemize}
\section{Future Work}
\subsection{Full System Adaptation}
Given the nature of the existing software system, in effect I wrote an adapter for the Variome Corpus, transforming the Variome Corpus Annotations to a format that the ASM system can work with. This is, of course, far from the best solution. However, the ASM system was originally developed solely for an event extraction shared task under time constraints, without much expectation that the system might be adapted on day. As a result, the system annotation format hard-coded in strings and a lot of implicit programming logic such as the involvement of trigger spread out across many different methods, to the extent that all of my efforts to change the code have failed. The main reason for the failure is my inability to detect all the methods and classes where changes should be made, resulting in unexpected system behavior. While is natural and understandable for academic software like this to be one-and-done after use in the shared task, I think it is in the interest of future system users and developers to have a well-documented, adequately extensible system in place, with room for tweaking algorithms and file formats with parameters. Ideally, the shortest path kernel will expose interfaces to users, allowing them to adjust what is happening under the hood with a few parameters. For instance, the existence of trigger should be optional and can be set with a parameter applied to the both the rule learning process and event extraction process. In fact, even in the shared task 2013, the existence of trigger for events like relations or coherence is optional. Next, annotation format should be parametrized, and the system should establish a relationship between user-defined entity and relation types its the entity and relation class. Moreover, the named entity recognition could be a dispatchable unit of the system too, with options to use user-given entities or the output of a named-entity recognizer. A perfect scenario would be one in which the user can input a configuration schema indicating annotation formats, entity types, entity output(whether from the annotation file or from a named entity recognizer), event/relations types and the location of textual data. That way the system can be used for different kinds of tasks, possibly even beyond the domain of biomedical text mining and provide valuable feedback for the plausibility of the approximate subgraph matching paradigm.   
\subsection{Relation Type Fine-Tuning}
Currently, the system only has two relations types, a ``has'' relation and a ``relatedTo'' relation. However, a patient-has-disease relation is semantically quite different from a gene-has-mutation relation, despite both of them being chunked to a single relation type called ``has''. One might be tempted to have fine-grained relation types and incorporate the entity types into the relation, such as a separate ``patient-cohort-has-disease'' and a separate  ``gene-has-mutation'' instead of a ``has'' relation for both. Nevertheless, this would cause the training set to become extremely sparse and the system learning behavior could change drastically. The correlation between granularity of event types and system performance has yet to be explored.  
\subsection{The Parser Effect}
This project uses the exact same preprocessing pipeline as \cite{mackinlay2013extracting}. However, \citet*{mackinlay2013extracting} concluded that changing the parser from the one originally used in \cite{liu2013approximate} has limited recall to an effect that can not be offset by increasing the amount of training data. It was suggested that the longer dependency graph produced by the \emph{clearnlp} parser is harder to generalize. The effect of parser on relation extraction task for the Variome Corpus is yet to be investigated. Again, the parser should be a loosely coupled component of the system too, such that the effect of different dependency parsers can be explored more easily.
\subsection{Parameter Tuning}
Once with an end-to-end system running on the Variome Corpus, the parameters such as subgraph weights, thresholds, rule set optimization aggressiveness need to be tuned for the training set and test parameter tuning on the test set.
\subsection{Combination of Kernels}
A significant limitation of the ASM based approach is the lower recall compared with other systems. 
Successful general literature mining at the semantic level might require a combination of many approaches. The shortest path kernel should be a loosely coupled component of the system and replaceable or by other kernels or a combination of kernels. 
\subsection{The Abstract Effect}
Many earlier biomedical text mining approaches only process abstracts of articles. The rationale is that abstract would contain a summary of the whole article and the important relations that it contains. In the Variome Corpus the full-text articles are splitted into sections such as abstract, introduction, conclusion, etc. With this in mind, one might be careful in how to distribute data for training, development and testing to avoid over-fitting in the future.
