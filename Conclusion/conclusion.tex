%*******************************************************************************
%*********************************** First Introduction *****************************
%*******************************************************************************
%\chapter{Introduction}  %Title of the First Introduction
%\chapter*{Introduction}
\chapter{Conclusion}  %Title of the First Introduction
%\addcontentsline{tableofcontents}{chapter}{introduction}

\ifpdf
    \graphicspath{{Conclusion/Figs/Raster/}{Conclusion/Figs/PDF/}{Conclusion/Figs/}}
\else
    \graphicspath{{Conclusion/Figs/Vector/}{Conclusion/Figs/}}
\fi

%********************************** %First Section  **************************************
\section{Conclusion} %Section - 1.1 	
We have explored the possibility of adapting an event-extraction system in relation extraction tasks for the Variome Corpus. 

\section{Contributions}
The contributions of the research in this thesis are as follows:
\begin{itemize}
	\item \emph{The first attempt to use the system outside original system developers.} While this may seem like an easy task, a lot of the time for this project has been devoted to resolving dependencies, and debugging scripts to first have an end-to-end system, even for the original shared task data set.
	\item \emph{The first attempt for literature mining of the Variome Corpus} To our knowledge, this is the first text mining attempt to the Variome Corpus. 
\end{itemize}
\section{Future Work}
\subsection{Full System Adaptation}
Given the nature of the existing software system, I effectively wrote an adapter for the Variome Corpus, transforming the current data set to a format that the system is expecting. This is, of course, far from the best solution. However, system was developed solely for the purpose of event extraction tasks, and has numerous constraints and data format expectations as hard-coded strings and methods, to the extent that all of my efforts to change the code have failed. It is natural and understandable for academic software like this to be one-and-done for things like the shared task, yet I think it is in the interest of future system users and developers to have a well-documented, adequately extensible system in place, with room for tweaking algorithms and file formats with parameters. Ideally, the shortest path kernel will expose interfaces for users to adjust what is happening under the hood with a few parameters. For instance, the existence of trigger should be optional and can be set with a parameter applied to the both rule learning process and event extraction process. Even in the shared task 2013, the existence of triggers for events like relations or coherence is optional. Next, annotation format should be parametrized, the system should establish a relationship between user-set entity and relation format and the entities and relations. Moreover, the named entity recognition could be a dispatch-able unit of the system too with options of a user-given entities or the output of a named-entity recognizer. A perfect scenarios would be that the user can input a configuration schema indicating annotation formats, entity types, entity output(whether from the annotation file or from a named entity recognizer), event/relations types and textual data. That way the system can be used for different kinds of tasks, possibly even beyond the domain of biomedical text mining and provide valuable feedback for the plausibility of the approximate subgraph matching paradigm.   
\subsection{Relation Type Fine-Tuning}
Currently, the system only has two relations types, a ``has'' relation and a ``relatedTo'' relation. However, a patient-has-disease relation is semantically quite different from a gene-has-mutation relation, despite both of them being chunked to a single relation type called ``has''. One might be tempted to have fine-grained relation types and incorporate the entity types into the relation, such as a separate ``patient-cohort-has-disease'' and a separate  ``gene-has-mutation'' instead of a ``has'' relation for both. Nevertheless, this would cause the training set to become extremely sparse and the system learning behavior could change drastically. The correlation between granularity of event types and system performance has yet to be explored.  
\subsection{Parser Effect}
This project uses the exact same preprocessing pipeline as
cite{Extracting Biomedical Events and Modifications Using Subgraph Matching with Noisy Training Data, namely the JULIE Sentence Boundary Detector, or JSBD (Tomanek et al., 2007), We then parse using a version of clearnlp1 (Choi and McCal- lum, 2013), a successor to ClearParser (Choi and Palmer, 2011),  However, it
} concluded that changing the parser from the one originally in \cite{liu2013approximate} has limited recall to an effect that can not be offset by increasing amount of training data, mainly because the longer dependency graph produced by the clearnlp parser is harder to generalize. The parser effect regarding relation extraction task for the Variome Corpus is yet to be explained. The parser, too should be a loosely coupled component of the system such that the effect of different dependency parsers can be explored easily.  
\subsection{Parameter Tuning}
The parameters such as subgraph weights, thresholds,  rule set optimization aggressiveness for the training set and test them on the test set. 
\subsection{Kernel Combining}
A significant limitation of the ASM based approach is the lower recall compared with other systems. 
Successful general literature mining at the semantic level might require a combination of many approaches. The shortest path kernel should be a loosely coupled component of the system and replaceable or by other kernels or a combination of kernels. 
